
### AI chat模块
- 不同的Agent有不同的提示词，上下文注入

- SSE流式传输，麻烦的点在于EventSource有状态，并且需要持续建立长连接，以及网络不好会出现波动。


- “高质量Prompt模板库”是怎么回事？
    - mcp+markdown的库，可以直接复制，也可以通过mcp进行检索
    - 通过mcp进行检索相关文件


- “记忆库(Memory Bank)”是如何工作的？
    - 形成“Prompt-代码实现”的配对数据。
    - 我们会先用用户的Prompt去“记忆库”中做一次向量相似度检索。如果找到高度相似的历史交互，我们会把之前的成功代码作为高质量的示例(Few-shot Example)注入到新的Prompt中，极大地提升了生成代码的准确性和一致性。

- 你提到的上下文注入，如果项目文件很大，上下文很容易就超出模型的Token限制了，你们是如何解决这个问题的？做了哪些截断或摘要策略？
    - 智能分块 (Chunking)：当用户选择一个文件或目录作为上下文时，我们不会直接读取全部内容。而是在后台用一个基于AST（抽象语法树）的解析器，将代码拆分成更小的、有意义的块（Chunk），比如一个函数、一个React组件的class或function定义等。每个块都保持了其上下文的完整性。
    - 向量化与检索：我们将这些代码块转换成向量，并存在一个临时的内存向量数据库中。当用户发起提问时，我们会同时对用户的问题（Prompt）也进行向量化，然后用它去检索最相关的Top-K（比如Top 5）个代码块。

“记忆库”听起来很像Fine-tuning的数据准备过程。你认为它和直接对模型进行微调相比，各有什么优劣？

迭代效率提升50%是如何统计的？是基于Story Point、工时，还是其他的量化指标？

#### 实现了打字机效果：
    - 方案：
        - innerHTML，然后批量setState；是一个不太好的方案
        - 直接使用setState
        - 使用raf，适用于高性能场景，一般是过度优化


#### 为什么采用 Server-Sent Events (SSE) 结合 Fetch API 的 ReadableStream（而不是EventSource）？
核心需求：：能够发起一个携带复杂数据（prompt、上下文等）的POST请求，并以流的形式接收响应，从而实现打字机效果

方案对比：
    - EventSource
        - 好处：
            - api简洁易用
            - 会自动重连
        - 缺点：
            - 不能用POST请求, 所以不方便发送上下文
            - 也无法自定义请求头
    - axios
        - 不支持响应流，只能整体返回resolve
    - fetch api
        - 支持流式返回，麻烦一点点，但是需要手动重连

##### 手动重连的方案：带抖动的指数退避
就是重连操作间隔指数增加：2、4、8、16等等。
同时每次都加一点 Random数据，避免客户端同时断线又同时重连。


#### 追问：

- 响应延迟降低了约80%，如何衡量？
    - 通过performance api进行测量，基于历史数据，网络良好的情况下，跑通链路并回复，需要5秒。
    - 我们在循环接收时，

- 你提到将答案采信率提升了50%，这个数据是如何度量的？你们做了A/B测试吗？还是通过用户调研问卷？
    - 通过埋点以及A/B测试来反映方案是否更有效
    - 测试”回复率“, 比如复制相关的文本，或者点赞
        - A组（无来源）的“回答复制率”稳定在10%左右。
        - B组（有来源）的“回答复制率”则达到了15%。
- 在处理流式响应时，如果一个数据块正好把一个JSON元数据对象从中间截断了，你的前端解析器要如何处理才不会崩溃并能正确拼接？

- RAG的来源可视化方案听起来不错，但如果来源文档非常多，比如十几个，UI上会很杂乱。你对这种情况做了哪些优化或取舍？


- 你提到的上下文注入，如果项目文件很大，上下文很容易就超出模型的Token限制了，你们是如何解决这个问题的？做了哪些截断或摘要策略？

- “记忆库”听起来很像Fine-tuning的数据准备过程。你认为它和直接对模型进行微调相比，各有什么优劣？

- 迭代效率提升50%是如何统计的？是基于Story Point、工时，还是其他的量化指标？





